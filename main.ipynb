{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdef87f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "import utils\n",
    "import config\n",
    "\n",
    "from llm import llm_wrapper\n",
    "from embedding import embedder\n",
    "from chain import rag_chain\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b048a32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(utils)\n",
    "importlib.reload(config)\n",
    "importlib.reload(llm_wrapper)\n",
    "importlib.reload(embedder)\n",
    "importlib.reload(rag_chain)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da7ca3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Embedding Model\n",
      "--------------------\n",
      "Initializing LLM Model\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09edbeb245224ba4ab37976eaab1d9d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Retrieval Chain\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "## Initialization of sub-components\n",
    "my_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "## Dataset Loader\n",
    "embedder_name = config.EMBED_MODEL\n",
    "data_dir = config.DATA_DIR\n",
    "documents = config.DOCUMENTS\n",
    "\n",
    "# Embedder class, which is responsible to init, create, load dataset\n",
    "my_embedder = embedder.Embedder(embedder_name, data_dir, documents, chunk_length=512, overlap=32, save_vec_db=True)\n",
    "\n",
    "doc_index = 0 # the index of the document that we want to do RAG\n",
    "loaded_doc = my_embedder.pdf_data_loader(doc_index)\n",
    "cleaned_docs = utils.clean_business_conduct_policy(loaded_doc, n_remove_first_lines=3, n_discard_pages=[1, 2, 20])\n",
    "chunks = my_embedder.text_splitter(cleaned_docs)\n",
    "vector_db, retriever = my_embedder.create_vector_database(chunks, top_k_doc=5)\n",
    "\n",
    "## LLM Loader\n",
    "base_model_id = config.LLM_MODEL_ID\n",
    "my_llm_wrapper = llm_wrapper.SmolLLMWrapper(base_model_id, max_length=256, \n",
    "                                            temperature=0.3, top_p=0.9, top_k=50, repetition_penalty=1.2, do_sample=True, truncation=True)\n",
    "llm, llm_tokenizer = my_llm_wrapper.get_llm()\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=llm, tokenizer=llm_tokenizer, max_new_tokens=256,\n",
    "                 temperature=0.3, top_p=0.9, top_k=50, repetition_penalty=1.2, do_sample=True, truncation=True)\n",
    "llm_hg_pipeline = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "## RAG Chain Loader\n",
    "rag_chain_train = rag_chain.RAGChainBuilder(llm_hg_pipeline, retriever)\n",
    "qa_bot_chain = rag_chain_train.build_chain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dff24ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        ### Context:\n",
      "        •  Speak up. If you see or hear of any violation of Apple’s Business Conduct Policy, other Apple policies, or legal or \n",
      "regulatory requirements, you must notify either your manager, People Team, Legal, or Business Conduct. \n",
      "•  Use good judgment and ask questions. Apply Apple’s principles of business conduct, and review our policies and legal \n",
      "requirements. When in doubt about how to proceed, discuss it with your manager, your People Business Partner, Legal, or \n",
      "Business Conduct.\n",
      "\n",
      "•  Reported or participated in the investigation of a potential violation of our policies or the law; or\n",
      "• Engaged in legally protected activity, including related to leaves of absence or job accommodations, or forming or joining \n",
      "(or refraining from joining) labor organizations of an employee’s choice in a lawful manner.\n",
      "\n",
      "Everything we do is a reflection of Apple. We expect you to:\n",
      "• Follow the Policy and exhibit appropriate workplace behavior. Comply with the letter and spirit of Apple’s Business \n",
      "Conduct Policy and all applicable legal requirements. Any failure to exhibit ethical or appropriate workplace behavior or \n",
      "to comply with Apple’s Business Conduct Policy—or failure to report a policy, regulatory, or legal violation—may result in \n",
      "disciplinary action, up to and including termination of employment.\n",
      "\n",
      "Business Conduct. \n",
      "You are also required to fully cooperate in any Apple investigation and safeguard the integrity of the investigation.\n",
      "Reporting a Concern\n",
      "To report a concern or ask a question about Apple’s Business Conduct Policy, you can contact Business Conduct by phone, \n",
      "email, or web form. For contact details, visit the Business Conduct website or the Resources section at the end of this\n",
      "\n",
      "policies for your region.\n",
      "If you believe you have been harassed or discriminated against, or have witnessed such behavior, visit Reporting a \n",
      "Concern. We encourage you to report the incident using any avenue with which you feel most comfortable, including your \n",
      "Apple manager, People Support, your People Business Partner, or Business Conduct.\n",
      "We also do not tolerate workplace violence of any kind. For more information, see the Workplace Violence Policy.\n",
      "Respect\n",
      "\n",
      "        ### Instruction:\n",
      "        You are a helpful assistant. Answer the question based on the above context provided.        \n",
      "\n",
      "        ### Question:\n",
      "        What should an employee do if they suspect a policy violation?\n",
      "\n",
      "        ### Answer:\n",
      "        According to the context, an employee should notify their manager, People Team, Legal, or Business Conduct about any suspicion of a policy violation.\n"
     ]
    }
   ],
   "source": [
    "# Sample Test Question with RAG - LangChain Pipeline\n",
    "question = \"What should an employee do if they suspect a policy violation?\"\n",
    "\n",
    "# Perform a query using the QA bot chain\n",
    "response = qa_bot_chain.run(question)\n",
    "\n",
    "print(f\"{response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5fba17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question Template (Input of LLM):\n",
      "[{'role': 'user', 'content': 'What should an employee do if they suspect a policy violation?'}]\n",
      "--------------------\n",
      "LLM Response:\n",
      "* **Report the violation to their supervisor or manager.**\n",
      "* **Document the incident.**\n",
      "* **Gather evidence.**\n",
      "* **Talk to the person involved.**\n",
      "* **Follow up with the supervisor or manager.**\n",
      "\n",
      "**Additional tips for reporting a policy violation:**\n",
      "\n",
      "* **Be objective and factual in your report.**\n",
      "* **Provide as much detail as possible.**\n",
      "* **Be aware of your company's reporting procedures.**\n",
      "* **Do not make any promises or guarantees.**\n",
      "* **Be respectful and professional in your communication.**\n"
     ]
    }
   ],
   "source": [
    "# Sample Test Question with RAW LLM. \n",
    "# IT MEANS THE OUTPUT IS SOLELY BASED ON WITOUT RAG - LangChain Pipeline\n",
    "question = \"What should an employee do if they suspect a policy violation?\"\n",
    "\n",
    "# Perform a query using pure LLM\n",
    "raw_llm_templaate = my_llm_wrapper.llm_template(question, include_system=False)\n",
    "\n",
    "print(f\"Question Template (Input of LLM):\\n{raw_llm_templaate}\")\n",
    "print(\"-\"*20)\n",
    "\n",
    "raw_llm_output = my_llm_wrapper.llm_generate_output(raw_llm_templaate)\n",
    "cleaned_raw_llm_output = utils.clean_gemma_response(raw_llm_output, question)\n",
    "\n",
    "\n",
    "print(f\"LLM Response:\\n{cleaned_raw_llm_output}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
