# QA BOT with RAG (LANGCHAIN), HuggingFace LLM, and User Interface (Gradio)

A modular and efficient Retrieval-Augmented Generation (RAG) system with document upload, vector search, and question answering UI powered by Gradio and LangChain.

---

## Project Overview

This project allows users to:
- Upload PDF documents.
- Automatically parse and split documents into semantic chunks.
- Create a FAISS-based vector database using sentence-transformer embeddings.
- Ask questions through a Gradio-based UI.
- Get answers generated by a lightweight LLM (e.g., Gemma-2B-IT) via LangChain retrieval pipeline.

The system is fully modular with components for document loading, embedding generation, vector storage, retrieval, and LLM-based answer generation.

---

## Project Architecture


```bash
QA_BOT_RAG_LANGCHAIN/
│
├── chain/                  # LangChain RAG chain builder
│   └── rag_chain.py
│
├── data/                   # PDF and question datasets
│
├── db/                     # Saved vector databases (FAISS)
│
├── embedding/              # Embedder logic (Sentence Transformers, vectorization)
│   └── embedder.py
│
├── llm/                    # LLM wrapper logic (HuggingFace Transformers models)
│   └── llm_wrapper.py
│
├── main.py                 # Gradio App UI
├── main.ipynb              # Optional Notebook version
├── utils.py                # Utility functions (text cleaning etc.)
├── rag_evaluation.ipynb    # ROUGE-based evaluation comparison between RAG and Raw LLM Outputs
├── config.py               # Project-wide configurations
├── requirements.txt        # Required libraries
├── README.md
├── evaluation_report.md    # Evaluation score analysis in terms of precision-recall-F1 and their impacts 
└── .gitignore              # Git ignore settings
