{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d46524e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "import utils\n",
    "import config\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "from llm import llm_wrapper\n",
    "from embedding import embedder\n",
    "from chain import rag_chain\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4e5f34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(utils)\n",
    "importlib.reload(config)\n",
    "importlib.reload(llm_wrapper)\n",
    "importlib.reload(embedder)\n",
    "importlib.reload(rag_chain)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d16ca761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Embedding Model\n",
      "--------------------\n",
      "Initializing LLM Model\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fc1c25a0e2d4167942d660406d69844",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Retrieval Chain\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "## Initialization of sub-components\n",
    "my_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "## Dataset Loader\n",
    "embedder_name = config.EMBED_MODEL\n",
    "data_dir = config.DATA_DIR\n",
    "documents = config.DOCUMENTS\n",
    "\n",
    "# Embedder class, which is responsible to init, create, load dataset\n",
    "my_embedder = embedder.Embedder(embedder_name, data_dir, documents, chunk_length=512, overlap=32, save_vec_db=True)\n",
    "\n",
    "doc_index = 0 # the index of the document that we want to do RAG\n",
    "loaded_doc = my_embedder.pdf_data_loader(doc_index)\n",
    "cleaned_docs = utils.clean_business_conduct_policy(loaded_doc, n_remove_first_lines=3, n_discard_pages=[1, 2, 20])\n",
    "chunks = my_embedder.text_splitter(cleaned_docs)\n",
    "vector_db, retriever = my_embedder.create_vector_database(chunks, top_k_doc=5)\n",
    "\n",
    "## LLM Loader\n",
    "base_model_id = config.LLM_MODEL_ID\n",
    "my_llm_wrapper = llm_wrapper.SmolLLMWrapper(base_model_id, max_length=256, \n",
    "                                            temperature=0.3, top_p=0.9, top_k=50, repetition_penalty=1.2, do_sample=True, truncation=True)\n",
    "llm, llm_tokenizer = my_llm_wrapper.get_llm()\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=llm, tokenizer=llm_tokenizer, max_new_tokens=256,\n",
    "                 temperature=0.3, top_p=0.9, top_k=50, repetition_penalty=1.2, do_sample=True, truncation=True)\n",
    "llm_hg_pipeline = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "## RAG Chain Loader\n",
    "rag_chain_train = rag_chain.RAGChainBuilder(llm_hg_pipeline, retriever)\n",
    "qa_bot_chain = rag_chain_train.build_chain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a852fd59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        ### Context:\n",
      "        •  Speak up. If you see or hear of any violation of Apple’s Business Conduct Policy, other Apple policies, or legal or \n",
      "regulatory requirements, you must notify either your manager, People Team, Legal, or Business Conduct. \n",
      "•  Use good judgment and ask questions. Apply Apple’s principles of business conduct, and review our policies and legal \n",
      "requirements. When in doubt about how to proceed, discuss it with your manager, your People Business Partner, Legal, or \n",
      "Business Conduct.\n",
      "\n",
      "•  Reported or participated in the investigation of a potential violation of our policies or the law; or\n",
      "• Engaged in legally protected activity, including related to leaves of absence or job accommodations, or forming or joining \n",
      "(or refraining from joining) labor organizations of an employee’s choice in a lawful manner.\n",
      "\n",
      "Everything we do is a reflection of Apple. We expect you to:\n",
      "• Follow the Policy and exhibit appropriate workplace behavior. Comply with the letter and spirit of Apple’s Business \n",
      "Conduct Policy and all applicable legal requirements. Any failure to exhibit ethical or appropriate workplace behavior or \n",
      "to comply with Apple’s Business Conduct Policy—or failure to report a policy, regulatory, or legal violation—may result in \n",
      "disciplinary action, up to and including termination of employment.\n",
      "\n",
      "Business Conduct. \n",
      "You are also required to fully cooperate in any Apple investigation and safeguard the integrity of the investigation.\n",
      "Reporting a Concern\n",
      "To report a concern or ask a question about Apple’s Business Conduct Policy, you can contact Business Conduct by phone, \n",
      "email, or web form. For contact details, visit the Business Conduct website or the Resources section at the end of this\n",
      "\n",
      "policies for your region.\n",
      "If you believe you have been harassed or discriminated against, or have witnessed such behavior, visit Reporting a \n",
      "Concern. We encourage you to report the incident using any avenue with which you feel most comfortable, including your \n",
      "Apple manager, People Support, your People Business Partner, or Business Conduct.\n",
      "We also do not tolerate workplace violence of any kind. For more information, see the Workplace Violence Policy.\n",
      "Respect\n",
      "\n",
      "        ### Instruction:\n",
      "        You are a helpful assistant. Answer the question based on the above context provided.        \n",
      "\n",
      "        ### Question:\n",
      "        What should an employee do if they suspect a policy violation?\n",
      "\n",
      "        ### Answer:\n",
      "        An employee should notify their manager, People Team, Legal, or Business Conduct about any suspicion of a policy violation.\n"
     ]
    }
   ],
   "source": [
    "# Sample Test Question with RAG - LangChain Pipeline\n",
    "question = \"What should an employee do if they suspect a policy violation?\"\n",
    "\n",
    "# Perform a query using the QA bot chain\n",
    "response = qa_bot_chain.run(question)\n",
    "cleaned_response = utils.clean_gemma_chain_response(response, keyword='### Answer:')\n",
    "\n",
    "print(f\"{response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7b12d91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English</th>\n",
       "      <th>Turkish</th>\n",
       "      <th>English Answer</th>\n",
       "      <th>Turkish Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are the core principles of Apple's Busine...</td>\n",
       "      <td>Apple'ın İş Ahlakı Politikasının temel ilkeler...</td>\n",
       "      <td>Apple's core principles are: Honesty - demonst...</td>\n",
       "      <td>Apple'ın temel ilkeleri şunlardır: Dürüstlük -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What ethical behavior does Apple expect from i...</td>\n",
       "      <td>Apple, çalışanlarından hangi etik davranışları...</td>\n",
       "      <td>Apple expects employees to follow the Policy a...</td>\n",
       "      <td>Apple çalışanlarından Politikaya uymalarını ve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What responsibilities do employees have regard...</td>\n",
       "      <td>Çalışanların yasalara ve düzenlemelere uyum ko...</td>\n",
       "      <td>Employees must comply with Apple's Business Co...</td>\n",
       "      <td>Çalışanlar Apple'ın İş Ahlakı Politikası'na ve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             English  \\\n",
       "0  What are the core principles of Apple's Busine...   \n",
       "1  What ethical behavior does Apple expect from i...   \n",
       "2  What responsibilities do employees have regard...   \n",
       "\n",
       "                                             Turkish  \\\n",
       "0  Apple'ın İş Ahlakı Politikasının temel ilkeler...   \n",
       "1  Apple, çalışanlarından hangi etik davranışları...   \n",
       "2  Çalışanların yasalara ve düzenlemelere uyum ko...   \n",
       "\n",
       "                                      English Answer  \\\n",
       "0  Apple's core principles are: Honesty - demonst...   \n",
       "1  Apple expects employees to follow the Policy a...   \n",
       "2  Employees must comply with Apple's Business Co...   \n",
       "\n",
       "                                      Turkish Answer  \n",
       "0  Apple'ın temel ilkeleri şunlardır: Dürüstlük -...  \n",
       "1  Apple çalışanlarından Politikaya uymalarını ve...  \n",
       "2  Çalışanlar Apple'ın İş Ahlakı Politikası'na ve...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Read <Question-Answer> Pair\n",
    "qa_doc_path = os.path.join(config.DATA_DIR, \"apple_business_conduct_questions-answers_bilingual.csv\")\n",
    "qa_df = pd.read_csv(qa_doc_path)\n",
    "\n",
    "qa_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76c8921a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▋    | 9/16 [00:56<00:44,  6.42s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "100%|██████████| 16/16 [02:03<00:00,  7.75s/it]\n"
     ]
    }
   ],
   "source": [
    "finalized_en_questions = []\n",
    "finalized_en_gt_responses = []\n",
    "finalized_en_chain_responses = []\n",
    "finalized_en_raw_responses = []\n",
    "\n",
    "\n",
    "for i in tqdm(range(len(qa_df))):\n",
    "    eng_question = qa_df.iloc[i, 0]\n",
    "    eng_answer = qa_df.iloc[i, 2]\n",
    "\n",
    "    tr_question = qa_df.iloc[i, 1]\n",
    "    tr_answer = qa_df.iloc[i, 3]\n",
    "\n",
    "    ## Get an answer using RAG - LangChain Pipeline\n",
    "    en_response_chain = qa_bot_chain.run(eng_question)\n",
    "    en_cleaned_response_chain = utils.clean_gemma_chain_response(response, keyword='### Answer:')\n",
    "\n",
    "    ## Get an answer using raw LLM model\n",
    "    en_llm_templaate_raw = my_llm_wrapper.llm_template(eng_question, include_system=False)\n",
    "    en_llm_output_raw = my_llm_wrapper.llm_generate_output(en_llm_templaate_raw)\n",
    "    en_cleaned_response_raw = utils.clean_gemma_response(en_llm_output_raw, question)\n",
    "\n",
    "    finalized_en_questions.append(eng_question)\n",
    "    finalized_en_gt_responses.append(eng_answer)\n",
    "    finalized_en_chain_responses.append(en_cleaned_response_chain)\n",
    "    finalized_en_raw_responses.append(en_cleaned_response_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0b30d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rouge(prediction: str, reference: str):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(reference, prediction)\n",
    "    return scores\n",
    "\n",
    "def batch_calculate_rouge(predictions: list, references: list):\n",
    "    assert len(predictions) == len(references), \"Predictions and references must be the same length.\"\n",
    "\n",
    "    rouge1_p, rouge1_r, rouge1_f = [], [], []\n",
    "    rouge2_p, rouge2_r, rouge2_f = [], [], []\n",
    "    rougel_p, rougel_r, rougel_f = [], [], []\n",
    "\n",
    "    for pred, ref in zip(predictions, references):\n",
    "        scores = calculate_rouge(pred=pred, reference=ref)\n",
    "\n",
    "        rouge1_p.append(scores['rouge1'].precision)\n",
    "        rouge1_r.append(scores['rouge1'].recall)\n",
    "        rouge1_f.append(scores['rouge1'].fmeasure)\n",
    "\n",
    "        rouge2_p.append(scores['rouge2'].precision)\n",
    "        rouge2_r.append(scores['rouge2'].recall)\n",
    "        rouge2_f.append(scores['rouge2'].fmeasure)\n",
    "\n",
    "        rougel_p.append(scores['rougeL'].precision)\n",
    "        rougel_r.append(scores['rougeL'].recall)\n",
    "        rougel_f.append(scores['rougeL'].fmeasure)\n",
    "\n",
    "    results = {\n",
    "        \"ROUGE-1\": {\n",
    "            \"Precision\": np.mean(rouge1_p),\n",
    "            \"Recall\": np.mean(rouge1_r),\n",
    "            \"F1\": np.mean(rouge1_f)\n",
    "        },\n",
    "        \"ROUGE-2\": {\n",
    "            \"Precision\": np.mean(rouge2_p),\n",
    "            \"Recall\": np.mean(rouge2_r),\n",
    "            \"F1\": np.mean(rouge2_f)\n",
    "        },\n",
    "        \"ROUGE-L\": {\n",
    "            \"Precision\": np.mean(rougel_p),\n",
    "            \"Recall\": np.mean(rougel_r),\n",
    "            \"F1\": np.mean(rougel_f)\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "for gt_response, chain_response, raw_response in zip(finalized_en_gt_responses, finalized_en_chain_responses, finalized_en_raw_responses):\n",
    "\n",
    "    chain_score = calculate_rouge(chain_response, gt_response)\n",
    "    raw_score = calculate_rouge(raw_response, gt_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1891942",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
